{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983034d1",
   "metadata": {},
   "source": [
    "Tacotron 2 from NVIDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bb1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dllogger (from -r Tacotron2/requirements.txt (line 7))\n",
      "  Cloning https://github.com/NVIDIA/dllogger (to revision v0.1.0) to /tmp/pip-install-lebf6wcd/dllogger_b92ef5a75e7e4306b909a499054a2436\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger /tmp/pip-install-lebf6wcd/dllogger_b92ef5a75e7e4306b909a499054a2436\n",
      "  Running command git checkout -q 26a0f8f1958de2c0c460925ff6102a4d2486d6cc\n",
      "  Resolved https://github.com/NVIDIA/dllogger to commit 26a0f8f1958de2c0c460925ff6102a4d2486d6cc\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 1)) (3.10.1)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 2)) (2.2.4)\n",
      "Requirement already satisfied: inflect in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 3)) (7.5.0)\n",
      "Requirement already satisfied: librosa in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 5)) (1.15.2)\n",
      "Requirement already satisfied: resampy==0.3.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: numba>=0.47 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from resampy==0.3.1->-r Tacotron2/requirements.txt (line 6)) (0.61.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from inflect->-r Tacotron2/requirements.txt (line 3)) (10.6.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from inflect->-r Tacotron2/requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from numba>=0.47->resampy==0.3.1->-r Tacotron2/requirements.txt (line 6)) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r Tacotron2/requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa->-r Tacotron2/requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa->-r Tacotron2/requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Clone the Tacotron2 repository \n",
    "import os\n",
    "if not os.path.exists(\"Tacotron2\"):\n",
    "    !git clone https://github.com/NVIDIA/tacotron2.git\n",
    "\n",
    "# Install requirements if needed (this may include additional dependencies)\n",
    "!pip install -r Tacotron2/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276245",
   "metadata": {},
   "source": [
    "LJ Speech dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12304a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: ['wavs', 'metadata.csv', 'README']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "dataset_tar = \"LJSpeech-1.1.tar.bz2\"\n",
    "dataset_dir = \"LJSpeech-1.1\"\n",
    "\n",
    "# Download dataset if not already downloaded\n",
    "# if not os.path.exists(dataset_tar):\n",
    "#     !wget --no-check-certificate {dataset_url}\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "if not os.path.exists(dataset_dir):\n",
    "    !tar -xjf {dataset_tar}\n",
    "\n",
    "print(\"Dataset ready:\", os.listdir(dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3592b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import abspath, dirname\n",
    "\n",
    "# Add the common directory to PATH so that tacotron2_common modules can be found\n",
    "current_dir = dirname(abspath(\"__file__\"))\n",
    "common_dir = abspath(current_dir + '/Tacotron2/tacotron2_common')\n",
    "if common_dir not in sys.path:\n",
    "    sys.path.insert(0, common_dir)\n",
    "\n",
    "# Import model components\n",
    "try:\n",
    "    from Tacotron2.tacotron2.model import Tacotron2 as Tacotron2Model\n",
    "    from Tacotron2.tacotron2.model import LocationLayer, Attention, Prenet, Postnet, Encoder, Decoder\n",
    "except ImportError as e:\n",
    "    print(\"ImportError:\", e)\n",
    "    # Optionally adjust the path if needed:\n",
    "    # sys.path.append(abspath(\"Tacotron2\"))\n",
    "    # from tacotron2.model import Tacotron2 as Tacotron2Model\n",
    "\n",
    "# Import PyTorch and other dependencies\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f929eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tacotron2(\n",
      "  (embedding): Embedding(148, 512)\n",
      "  (encoder): Encoder(\n",
      "    (convolutions): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (layers): ModuleList(\n",
      "        (0): LinearNorm(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): LinearNorm(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(768, 256)\n",
      "    (attention_layer): Attention(\n",
      "      (query_layer): LinearNorm(\n",
      "        (linear_layer): Linear(in_features=256, out_features=128, bias=False)\n",
      "      )\n",
      "      (memory_layer): LinearNorm(\n",
      "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): LinearNorm(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv): ConvNorm(\n",
      "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        )\n",
      "        (location_dense): LinearNorm(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(768, 256, bias=1)\n",
      "    (linear_projection): LinearNorm(\n",
      "      (linear_layer): Linear(in_features=768, out_features=80, bias=True)\n",
      "    )\n",
      "    (gate_layer): LinearNorm(\n",
      "      (linear_layer): Linear(in_features=768, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (postnet): Postnet(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1-3): 3 x Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the model with dummy hyperparameters for testing\n",
    "# Note: Replace these with appropriate hyperparameters for your setup.\n",
    "dummy_hparams = {\n",
    "    \"mask_padding\": True,\n",
    "    \"n_mel_channels\": 80,\n",
    "    \"n_symbols\": 148,  # This should correspond to the actual vocabulary size\n",
    "    \"symbols_embedding_dim\": 512,\n",
    "    \"encoder_kernel_size\": 5,\n",
    "    \"encoder_n_convolutions\": 3,\n",
    "    \"encoder_embedding_dim\": 512,\n",
    "    \"attention_rnn_dim\": 256,\n",
    "    \"attention_dim\": 128,\n",
    "    \"attention_location_n_filters\": 32,\n",
    "    \"attention_location_kernel_size\": 31,\n",
    "    \"n_frames_per_step\": 1,\n",
    "    \"decoder_rnn_dim\": 256,\n",
    "    \"prenet_dim\": 256,\n",
    "    \"max_decoder_steps\": 1000,\n",
    "    \"gate_threshold\": 0.5,\n",
    "    \"p_attention_dropout\": 0.1,\n",
    "    \"p_decoder_dropout\": 0.1,\n",
    "    \"postnet_embedding_dim\": 512,\n",
    "    \"postnet_kernel_size\": 5,\n",
    "    \"postnet_n_convolutions\": 5,\n",
    "    \"decoder_no_early_stopping\": False\n",
    "}\n",
    "\n",
    "model_instance = Tacotron2Model(\n",
    "    mask_padding=dummy_hparams[\"mask_padding\"],\n",
    "    n_mel_channels=dummy_hparams[\"n_mel_channels\"],\n",
    "    n_symbols=dummy_hparams[\"n_symbols\"],\n",
    "    symbols_embedding_dim=dummy_hparams[\"symbols_embedding_dim\"],\n",
    "    encoder_kernel_size=dummy_hparams[\"encoder_kernel_size\"],\n",
    "    encoder_n_convolutions=dummy_hparams[\"encoder_n_convolutions\"],\n",
    "    encoder_embedding_dim=dummy_hparams[\"encoder_embedding_dim\"],\n",
    "    attention_rnn_dim=dummy_hparams[\"attention_rnn_dim\"],\n",
    "    attention_dim=dummy_hparams[\"attention_dim\"],\n",
    "    attention_location_n_filters=dummy_hparams[\"attention_location_n_filters\"],\n",
    "    attention_location_kernel_size=dummy_hparams[\"attention_location_kernel_size\"],\n",
    "    n_frames_per_step=dummy_hparams[\"n_frames_per_step\"],\n",
    "    decoder_rnn_dim=dummy_hparams[\"decoder_rnn_dim\"],\n",
    "    prenet_dim=dummy_hparams[\"prenet_dim\"],\n",
    "    max_decoder_steps=dummy_hparams[\"max_decoder_steps\"],\n",
    "    gate_threshold=dummy_hparams[\"gate_threshold\"],\n",
    "    p_attention_dropout=dummy_hparams[\"p_attention_dropout\"],\n",
    "    p_decoder_dropout=dummy_hparams[\"p_decoder_dropout\"],\n",
    "    postnet_embedding_dim=dummy_hparams[\"postnet_embedding_dim\"],\n",
    "    postnet_kernel_size=dummy_hparams[\"postnet_kernel_size\"],\n",
    "    postnet_n_convolutions=dummy_hparams[\"postnet_n_convolutions\"],\n",
    "    decoder_no_early_stopping=dummy_hparams[\"decoder_no_early_stopping\"]\n",
    ")\n",
    "\n",
    "print(model_instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2c1da",
   "metadata": {},
   "source": [
    "Dataset and Dataloader set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85836788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['text', 'text_length', 'mel', 'mel_length', 'gate'])\n",
      "Text batch shape: torch.Size([4, 137])\n",
      "Mel batch shape: torch.Size([4, 80, 315])\n",
      "Gate batch shape: torch.Size([4, 315])\n",
      "tensor([236, 256, 315, 298])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa  # for audio loading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Set the path to your dataset\n",
    "DATASET_DIR = \"./LJSpeech-1.1/\"\n",
    "METADATA_PATH = os.path.join(DATASET_DIR, \"metadata.csv\")\n",
    "WAVS_DIR = os.path.join(DATASET_DIR, \"wavs/\")\n",
    "\n",
    "# Define a simple character vocabulary.\n",
    "# You can adjust the characters based on your needs.\n",
    "characters = \"abcdefghijklmnopqrstuvwxyz '!,?.\"\n",
    "# Create a mapping from character to an index.\n",
    "# Reserve 0 for padding.\n",
    "char2idx = {ch: idx+1 for idx, ch in enumerate(characters)}\n",
    "\n",
    "def text_to_sequence(text):\n",
    "    \"\"\"\n",
    "    Convert text into a sequence of indices using a character-based mapping.\n",
    "    Only characters in the vocabulary are used; all others are ignored.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    sequence = [char2idx.get(ch, 0) for ch in text if ch in char2idx]\n",
    "    return sequence\n",
    "\n",
    "# Custom function for computing mel spectrograms.\n",
    "# Replace this dummy implementation with your actual mel computation function.\n",
    "def compute_mel(wav_path, sr=22050, n_mel_channels=80):\n",
    "    # Load the audio file with librosa.\n",
    "    y, _ = librosa.load(wav_path, sr=sr)\n",
    "    # For demonstration: create a dummy mel spectrogram with random values.\n",
    "    # In practice, implement your real mel spectrogram computation here.\n",
    "    T = np.random.randint(200, 400)  # random time dimension between 200 and 400 frames.\n",
    "    mel = np.random.rand(n_mel_channels, T).astype(np.float32)\n",
    "    return mel\n",
    "\n",
    "# Define the LJ Speech Dataset class.\n",
    "class LJSpeechDataset(Dataset):\n",
    "    def __init__(self, metadata_path, wavs_dir, max_samples=None):\n",
    "        self.wavs_dir = wavs_dir\n",
    "        self.samples = []\n",
    "        # Read metadata.csv where each line is: ID|normalized text|original text.\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter='|')\n",
    "            for row in reader:\n",
    "                if len(row) >= 2:\n",
    "                    file_id = row[0].strip()\n",
    "                    text = row[1].strip()\n",
    "                    wav_path = os.path.join(wavs_dir, file_id + \".wav\")\n",
    "                    self.samples.append((file_id, text, wav_path))\n",
    "                    if max_samples is not None and len(self.samples) >= max_samples:\n",
    "                        break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_id, text, wav_path = self.samples[index]\n",
    "        # Tokenize the text using our character-level tokenizer.\n",
    "        sequence = text_to_sequence(text)\n",
    "        text_tensor = torch.LongTensor(sequence)\n",
    "        text_length = torch.LongTensor([len(sequence)])\n",
    "        \n",
    "        # Compute the mel spectrogram using the custom function.\n",
    "        mel = compute_mel(wav_path)\n",
    "        mel_tensor = torch.FloatTensor(mel)  # shape: (n_mel_channels, T)\n",
    "        mel_length = torch.LongTensor([mel_tensor.shape[1]])\n",
    "        \n",
    "        # Create a dummy gate signal.\n",
    "        # In a full implementation, the gate target indicates the end of the sequence.\n",
    "        gate_tensor = torch.FloatTensor([0])\n",
    "        \n",
    "        return {\n",
    "            'text': text_tensor,\n",
    "            'text_length': text_length,\n",
    "            'mel': mel_tensor,\n",
    "            'mel_length': mel_length,\n",
    "            'gate': gate_tensor\n",
    "        }\n",
    "\n",
    "# Define the collate function to pad sequences in a batch.\n",
    "def lj_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for the LJSpeech dataset.\n",
    "\n",
    "    Each sample in the batch is a dictionary with keys:\n",
    "      - 'text': 1D LongTensor of tokenized text.\n",
    "      - 'text_length': 1D LongTensor containing length of the text.\n",
    "      - 'mel': 2D FloatTensor of shape (n_mel_channels, T) (variable T).\n",
    "      - 'mel_length': 1D LongTensor containing number of frames in mel spectrogram.\n",
    "      - 'gate': 1D FloatTensor (dummy signal; last frame 1, others 0).\n",
    "\n",
    "    This function:\n",
    "      1. Sorts the batch by text length in descending order.\n",
    "      2. Pads the text sequences into one tensor.\n",
    "      3. Pads the mel spectrograms along the time dimension.\n",
    "      4. Creates a padded tensor for gate signals.\n",
    "    \"\"\"\n",
    "    # Sort batch by text length (descending order)\n",
    "    batch.sort(key=lambda x: x['text'].size(0), reverse=True)\n",
    "\n",
    "    # Extract the fields\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "    text_lengths = torch.tensor([sample['text'].size(0) for sample in batch], dtype=torch.long)\n",
    "\n",
    "    mels = [sample['mel'].transpose(0, 1) for sample in batch]  # transpose so shape becomes (T, n_mel_channels)\n",
    "    mel_lengths = torch.tensor([mel.size(0) for mel in mels], dtype=torch.long)\n",
    "    n_mel_channels = batch[0]['mel'].size(0)\n",
    "\n",
    "    # Pad text sequences (padding value 0 for padding token)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Pad mel spectrograms; note that these are variable in time dimension (T)\n",
    "    # We'll pad along the time axis so that each spectrogram becomes (T_max, n_mel_channels)\n",
    "    mels_padded = pad_sequence(mels, batch_first=True, padding_value=0)  # shape: (batch, T_max, n_mel_channels)\n",
    "    mels_padded = mels_padded.transpose(1, 2)  # shape: (batch, n_mel_channels, T_max)\n",
    "\n",
    "    # Create padded gate signals.\n",
    "    # For demonstration: all zeros, with the last valid frame set to 1.\n",
    "    max_mel_len = mels_padded.size(2)\n",
    "    gates = []\n",
    "    for sample in batch:\n",
    "        mel_len = sample['mel'].size(1)\n",
    "        # Create a gate tensor: zeros for frames 0...mel_len-2, and 1 for the final valid frame.\n",
    "        gate = torch.zeros(mel_len)\n",
    "        if mel_len > 0:\n",
    "            gate[-1] = 1\n",
    "        gates.append(gate)\n",
    "    gates_padded = pad_sequence(gates, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'text': texts_padded,          # LongTensor with shape (batch, max_text_length)\n",
    "        'text_length': text_lengths,     # LongTensor with shape (batch)\n",
    "        'mel': mels_padded,              # FloatTensor with shape (batch, n_mel_channels, max_mel_length)\n",
    "        'mel_length': mel_lengths,       # LongTensor with shape (batch)\n",
    "        'gate': gates_padded             # FloatTensor with shape (batch, max_mel_length)\n",
    "    }\n",
    "\n",
    "# Instantiate the dataset and dataloader.\n",
    "dataset = LJSpeechDataset(METADATA_PATH, WAVS_DIR, max_samples=100)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lj_collate_fn)\n",
    "\n",
    "# Check one batch to verify the setup.\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"Text batch shape:\", batch['text'].shape)\n",
    "print(\"Mel batch shape:\", batch['mel'].shape)\n",
    "print(\"Gate batch shape:\", batch['gate'].shape)\n",
    "print(batch['mel_length'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
