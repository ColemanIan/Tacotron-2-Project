{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983034d1",
   "metadata": {},
   "source": [
    "Tacotron 2 from NVIDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6bb1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dllogger (from -r Tacotron2/requirements.txt (line 7))\n",
      "  Cloning https://github.com/NVIDIA/dllogger (to revision v0.1.0) to /tmp/pip-install-z1ipp8ty/dllogger_e3abc3b8a37e4fdaae00f1df4f60ebaa\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger /tmp/pip-install-z1ipp8ty/dllogger_e3abc3b8a37e4fdaae00f1df4f60ebaa\n",
      "  Running command git checkout -q 26a0f8f1958de2c0c460925ff6102a4d2486d6cc\n",
      "  Resolved https://github.com/NVIDIA/dllogger to commit 26a0f8f1958de2c0c460925ff6102a4d2486d6cc\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 1)) (3.10.1)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 2)) (2.2.4)\n",
      "Collecting inflect (from -r Tacotron2/requirements.txt (line 3))\n",
      "  Downloading inflect-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting librosa (from -r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from -r Tacotron2/requirements.txt (line 5)) (1.15.2)\n",
      "Collecting resampy==0.3.1 (from -r Tacotron2/requirements.txt (line 6))\n",
      "  Downloading resampy-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numba>=0.47 (from resampy==0.3.1->-r Tacotron2/requirements.txt (line 6))\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->-r Tacotron2/requirements.txt (line 1)) (2.9.0.post0)\n",
      "Collecting more_itertools>=8.5.0 (from inflect->-r Tacotron2/requirements.txt (line 3))\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting typeguard>=4.0.1 (from inflect->-r Tacotron2/requirements.txt (line 3))\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->-r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa->-r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa->-r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->-r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from librosa->-r Tacotron2/requirements.txt (line 4)) (4.12.2)\n",
      "Collecting lazy_loader>=0.1 (from librosa->-r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->-r Tacotron2/requirements.txt (line 4))\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.47->resampy==0.3.1->-r Tacotron2/requirements.txt (line 6))\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r Tacotron2/requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa->-r Tacotron2/requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa->-r Tacotron2/requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r Tacotron2/requirements.txt (line 4)) (2025.1.31)\n",
      "Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading inflect-7.5.0-py3-none-any.whl (35 kB)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: dllogger\n",
      "  Building wheel for dllogger (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dllogger: filename=dllogger-0.1.0-py3-none-any.whl size=5686 sha256=f67d26037faa9562bad8a9a729a9ddf33bd74af85ef6dfba3f725d754d1a1857\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s1rwezdy/wheels/c7/6b/3d/99a9080ee5e8b6115d01c2b6ef5784a5ec7ebdc0a36592d2ac\n",
      "Successfully built dllogger\n",
      "Installing collected packages: typeguard, soxr, msgpack, more_itertools, llvmlite, lazy_loader, dllogger, audioread, soundfile, pooch, numba, inflect, resampy, librosa\n",
      "Successfully installed audioread-3.0.1 dllogger-0.1.0 inflect-7.5.0 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 more_itertools-10.6.0 msgpack-1.1.0 numba-0.61.2 pooch-1.8.2 resampy-0.3.1 soundfile-0.13.1 soxr-0.5.0.post1 typeguard-4.4.2\n"
     ]
    }
   ],
   "source": [
    "# Clone the Tacotron2 repository \n",
    "import os\n",
    "if not os.path.exists(\"Tacotron2\"):\n",
    "    !git clone https://github.com/NVIDIA/tacotron2.git\n",
    "\n",
    "# Install requirements if needed (this may include additional dependencies)\n",
    "!pip install -r Tacotron2/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276245",
   "metadata": {},
   "source": [
    "LJ Speech dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12304a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-12 15:08:37--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
      "Resolving data.keithito.com (data.keithito.com)... 185.93.1.245, 2400:52e0:1a00::1207:2\n",
      "Connecting to data.keithito.com (data.keithito.com)|185.93.1.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2748572632 (2.6G) [text/plain]\n",
      "Saving to: ‘LJSpeech-1.1.tar.bz2’\n",
      "\n",
      "LJSpeech-1.1.tar.bz 100%[===================>]   2.56G  69.9MB/s    in 51s     \n",
      "\n",
      "2025-04-12 15:09:28 (51.7 MB/s) - ‘LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n",
      "\n",
      "Dataset ready: ['wavs', 'metadata.csv', 'README']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "dataset_tar = \"LJSpeech-1.1.tar.bz2\"\n",
    "dataset_dir = \"LJSpeech-1.1\"\n",
    "\n",
    "# Download dataset if not already downloaded\n",
    "if not os.path.exists(dataset_tar):\n",
    "    !wget --no-check-certificate {dataset_url}\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "if not os.path.exists(dataset_dir):\n",
    "    !tar -xjf {dataset_tar}\n",
    "\n",
    "print(\"Dataset ready:\", os.listdir(dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3592b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import abspath, dirname\n",
    "\n",
    "# Add the common directory to PATH so that tacotron2_common modules can be found\n",
    "current_dir = dirname(abspath(\"__file__\"))\n",
    "common_dir = abspath(current_dir + '/Tacotron2/tacotron2_common')\n",
    "if common_dir not in sys.path:\n",
    "    sys.path.insert(0, common_dir)\n",
    "\n",
    "# Import model components\n",
    "try:\n",
    "    from Tacotron2.tacotron2.model import Tacotron2 as Tacotron2Model\n",
    "    from Tacotron2.tacotron2.model import LocationLayer, Attention, Prenet, Postnet, Encoder, Decoder\n",
    "except ImportError as e:\n",
    "    print(\"ImportError:\", e)\n",
    "    # Optionally adjust the path if needed:\n",
    "    # sys.path.append(abspath(\"Tacotron2\"))\n",
    "    # from tacotron2.model import Tacotron2 as Tacotron2Model\n",
    "\n",
    "# Import PyTorch and other dependencies\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f929eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tacotron2(\n",
      "  (embedding): Embedding(148, 512)\n",
      "  (encoder): Encoder(\n",
      "    (convolutions): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (prenet): Prenet(\n",
      "      (layers): ModuleList(\n",
      "        (0): LinearNorm(\n",
      "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): LinearNorm(\n",
      "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attention_rnn): LSTMCell(768, 256)\n",
      "    (attention_layer): Attention(\n",
      "      (query_layer): LinearNorm(\n",
      "        (linear_layer): Linear(in_features=256, out_features=128, bias=False)\n",
      "      )\n",
      "      (memory_layer): LinearNorm(\n",
      "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
      "      )\n",
      "      (v): LinearNorm(\n",
      "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
      "      )\n",
      "      (location_layer): LocationLayer(\n",
      "        (location_conv): ConvNorm(\n",
      "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "        )\n",
      "        (location_dense): LinearNorm(\n",
      "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_rnn): LSTMCell(768, 256, bias=1)\n",
      "    (linear_projection): LinearNorm(\n",
      "      (linear_layer): Linear(in_features=768, out_features=80, bias=True)\n",
      "    )\n",
      "    (gate_layer): LinearNorm(\n",
      "      (linear_layer): Linear(in_features=768, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (postnet): Postnet(\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1-3): 3 x Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ConvNorm(\n",
      "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        )\n",
      "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the model with dummy hyperparameters for testing\n",
    "# Note: Replace these with appropriate hyperparameters for your setup.\n",
    "dummy_hparams = {\n",
    "    \"mask_padding\": True,\n",
    "    \"n_mel_channels\": 80,\n",
    "    \"n_symbols\": 148,  # This should correspond to the actual vocabulary size\n",
    "    \"symbols_embedding_dim\": 512,\n",
    "    \"encoder_kernel_size\": 5,\n",
    "    \"encoder_n_convolutions\": 3,\n",
    "    \"encoder_embedding_dim\": 512,\n",
    "    \"attention_rnn_dim\": 256,\n",
    "    \"attention_dim\": 128,\n",
    "    \"attention_location_n_filters\": 32,\n",
    "    \"attention_location_kernel_size\": 31,\n",
    "    \"n_frames_per_step\": 1,\n",
    "    \"decoder_rnn_dim\": 256,\n",
    "    \"prenet_dim\": 256,\n",
    "    \"max_decoder_steps\": 1000,\n",
    "    \"gate_threshold\": 0.5,\n",
    "    \"p_attention_dropout\": 0.1,\n",
    "    \"p_decoder_dropout\": 0.1,\n",
    "    \"postnet_embedding_dim\": 512,\n",
    "    \"postnet_kernel_size\": 5,\n",
    "    \"postnet_n_convolutions\": 5,\n",
    "    \"decoder_no_early_stopping\": False\n",
    "}\n",
    "\n",
    "model_instance = Tacotron2Model(\n",
    "    mask_padding=dummy_hparams[\"mask_padding\"],\n",
    "    n_mel_channels=dummy_hparams[\"n_mel_channels\"],\n",
    "    n_symbols=dummy_hparams[\"n_symbols\"],\n",
    "    symbols_embedding_dim=dummy_hparams[\"symbols_embedding_dim\"],\n",
    "    encoder_kernel_size=dummy_hparams[\"encoder_kernel_size\"],\n",
    "    encoder_n_convolutions=dummy_hparams[\"encoder_n_convolutions\"],\n",
    "    encoder_embedding_dim=dummy_hparams[\"encoder_embedding_dim\"],\n",
    "    attention_rnn_dim=dummy_hparams[\"attention_rnn_dim\"],\n",
    "    attention_dim=dummy_hparams[\"attention_dim\"],\n",
    "    attention_location_n_filters=dummy_hparams[\"attention_location_n_filters\"],\n",
    "    attention_location_kernel_size=dummy_hparams[\"attention_location_kernel_size\"],\n",
    "    n_frames_per_step=dummy_hparams[\"n_frames_per_step\"],\n",
    "    decoder_rnn_dim=dummy_hparams[\"decoder_rnn_dim\"],\n",
    "    prenet_dim=dummy_hparams[\"prenet_dim\"],\n",
    "    max_decoder_steps=dummy_hparams[\"max_decoder_steps\"],\n",
    "    gate_threshold=dummy_hparams[\"gate_threshold\"],\n",
    "    p_attention_dropout=dummy_hparams[\"p_attention_dropout\"],\n",
    "    p_decoder_dropout=dummy_hparams[\"p_decoder_dropout\"],\n",
    "    postnet_embedding_dim=dummy_hparams[\"postnet_embedding_dim\"],\n",
    "    postnet_kernel_size=dummy_hparams[\"postnet_kernel_size\"],\n",
    "    postnet_n_convolutions=dummy_hparams[\"postnet_n_convolutions\"],\n",
    "    decoder_no_early_stopping=dummy_hparams[\"decoder_no_early_stopping\"]\n",
    ")\n",
    "\n",
    "print(model_instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2c1da",
   "metadata": {},
   "source": [
    "Dataset and Dataloader set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85836788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummyLJDataset(Dataset):\n",
    "    \"\"\"A dummy dataset for illustration. Replace with actual processing code.\n",
    "    Each sample returns (text_tensor, text_length, mel_tensor, gate_tensor, mel_length)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_dir, max_samples=100):\n",
    "        super(DummyLJDataset, self).__init__()\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.max_samples = max_samples\n",
    "        # In practice, you would parse the metadata file (e.g., metadata.csv) in LJ Speech\n",
    "        # and create a list of samples with file paths and corresponding text.\n",
    "        self.samples = [(\"This is a sample sentence.\", np.random.rand(80, 400).astype(np.float32)) for _ in range(max_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, mel = self.samples[idx]\n",
    "        # Dummy tokenization: convert characters to indices (for illustration)\n",
    "        # In practice, use a proper tokenizer and mapping.\n",
    "        text_tensor = torch.LongTensor([ord(c) for c in text])\n",
    "        text_length = torch.LongTensor([len(text_tensor)])\n",
    "        # Dummy mel spectrogram and gate: In practice, load precomputed features.\n",
    "        mel_tensor = torch.FloatTensor(mel)  # (n_mel_channels, T)\n",
    "        gate_tensor = torch.FloatTensor([0])  # Dummy gate signal\n",
    "        mel_length = torch.LongTensor([mel_tensor.shape[1]])\n",
    "        return text_tensor, text_length, mel_tensor, gate_tensor, mel_length\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "dataset = DummyLJDataset(dataset_dir=\"LJSpeech-1.1\", max_samples=100)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda batch: batch)\n",
    "\n",
    "print(\"Dummy dataset sample:\", dataset[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
